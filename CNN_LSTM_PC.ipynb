{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-LSTM-PC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "k6vFROPeO9SS"
      },
      "outputs": [],
      "source": [
        "#@title CNN-LSTM-PC\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Seed value\n",
        "seed_value= 0\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from keras.models import load_model\n",
        "import pandas as pd\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy import concatenate\n",
        "from scipy.stats import boxcox\n",
        "from math import exp\n",
        "from math import log\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from numpy import array\n",
        "from pandas import concat\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# invert a boxcox transform for one value\n",
        "def invert_boxcox(value, lam):\n",
        "  # log case\n",
        "  if lam == 0:\n",
        "    return exp(value)\n",
        "  # all other cases\n",
        "  return exp(log(lam * value + 1) / lam)\n",
        "\n",
        "# Convert series to supervised learning\n",
        "def sup_struct_future(dataset, target, start,window,horizon):\n",
        "  x_future = dataset[:,:-1]\n",
        "  end = None\n",
        "  X = []\n",
        "  y = []\n",
        "  X_f = []\n",
        "  start = start + window\n",
        "  if end is None:\n",
        "    end = len(dataset) - horizon\n",
        "  for i in range(start, end):\n",
        "    indices = range(i-window, i)\n",
        "    X.append(dataset[indices])\n",
        "    indicey = range(i+1, i+1+horizon)  \n",
        "    y.append(target[indicey])\n",
        "    X_f.append(x_future[indicey])\n",
        "  \n",
        "  x_past = np.array(X)\n",
        "  x_future_preds = np.array(X_f) \n",
        "  y_target = np.array(y)\n",
        "\n",
        "  x_past_reshaped = x_past.reshape(x_past.shape[0],x_past.shape[1]*x_past.shape[2])\n",
        "  x_future_preds_reshaped = x_future_preds.reshape(x_future_preds.shape[0],\n",
        "                                                  x_future_preds.shape[1]*x_future_preds.shape[2])\n",
        "  x_covs = np.concatenate([x_past_reshaped,x_future_preds_reshaped],axis=1)\n",
        "  return x_covs, y_target\n",
        "\n",
        "#Load county data\n",
        "mydata = pd.read_csv('nas.csv', header=0)\n",
        "dataIn = mydata[mydata.columns[1:8]] \n",
        "dateT = mydata[mydata.columns[0]]\n",
        "dateT = DataFrame(dateT)\n",
        "dateT['dateTime'] = dateT\n",
        "dateT.drop('Unnamed: 0', axis = 1, inplace = True)\n",
        "df_data = pd.concat((dateT, dataIn), axis=1)\n",
        "df_data = df_data.drop(df_data.columns[0], axis = 1)\n",
        "\n",
        "# Drop rows which contain any NaN values in all columns\n",
        "df_data = df_data.dropna( how='all')\n",
        "\n",
        "# Interpolate missing values\n",
        "df_data = df_data.interpolate(method ='linear', limit_direction ='both', limit = 10000, axis=0)\n",
        "\n",
        "# Split into train and test set\n",
        "train_size = int(len(df_data) * 0.8)\n",
        "train_data_1, test_data = df_data[0:train_size], df_data[train_size:]\n",
        "val_size = int(len(train_data_1) * 0.1)\n",
        "val_data = train_data_1[-val_size:]\n",
        "train_data = train_data_1[:train_size-val_size]\n",
        "\n",
        "# move target variable to the last column to align with surpervise learning struture\n",
        "new_train =  pd.DataFrame(train_data,columns=['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages'])\n",
        "new_test  =  pd.DataFrame(test_data,columns=['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages'])\n",
        "new_val   =  pd.DataFrame(val_data,columns=['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages'])\n",
        "raw_train =  pd.DataFrame(train_data,columns=['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages'])\n",
        "raw_test  =  pd.DataFrame(test_data,columns=['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages'])\n",
        "\n",
        "# power transform target variable to be gausian like :TRAIN\n",
        "box_power = new_train[['Total Outages']]\n",
        "box_power = np.array(box_power)\n",
        "box_power = box_power.reshape(box_power.shape[0]*1) \n",
        "transformed, lmbda = boxcox(box_power)\n",
        "transformed = transformed.reshape(-1,1) \n",
        "\n",
        "# Apply power transform to target validation data  :VAL\n",
        "box_power_val = new_val[['Total Outages']]\n",
        "box_power_val = np.array(box_power_val)\n",
        "box_power_val = box_power_val.reshape(box_power_val.shape[0]*1)  \n",
        "transformed_val = boxcox(box_power_val,lmbda)\n",
        "transformed_val = transformed_val.reshape(-1,1)  \n",
        "\n",
        "# Apply power transform to target validation data  :TEST\n",
        "box_power_test = new_test[['Total Outages']]\n",
        "box_power_test = np.array(box_power_test)\n",
        "box_power_test = box_power_test.reshape(box_power_test.shape[0]*1)  \n",
        "transformed_test = boxcox(box_power_test,lmbda)\n",
        "transformed_test = transformed_test.reshape(-1,1)  \n",
        "\n",
        "# Smothing targets\n",
        "dwAvg = 6 # 6hrs rolling avearge\n",
        "y_train_mean = pd.DataFrame(transformed)\n",
        "y_train_mean = y_train_mean.rolling(dwAvg).mean()\n",
        "y_train_mean = y_train_mean.interpolate(method ='linear', limit_direction ='both', limit = 100, axis=0)\n",
        "y_train_mean = np.array(y_train_mean)\n",
        "y_train_mean = y_train_mean.reshape(-1,1)\n",
        "new_train[['Total Outages']] = y_train_mean\n",
        "\n",
        "# Val fold            \n",
        "y_val_mean = pd.DataFrame(transformed_val)\n",
        "y_val_mean = y_val_mean.rolling(dwAvg).mean()\n",
        "y_val_mean = y_val_mean.interpolate(method ='linear', limit_direction ='both', limit = 100, axis=0)\n",
        "y_val_mean = np.array(y_val_mean)\n",
        "y_val_mean = y_val_mean.reshape(-1,1)\n",
        "new_val[['Total Outages']] = y_val_mean\n",
        "\n",
        "# Test fold            \n",
        "y_test_mean = pd.DataFrame(transformed_test)\n",
        "y_test_mean = y_test_mean.rolling(dwAvg).mean()\n",
        "y_test_mean = y_test_mean.interpolate(method ='linear', limit_direction ='both', limit = 100, axis=0)\n",
        "y_test_mean = np.array(y_test_mean)\n",
        "y_test_mean = y_test_mean.reshape(-1,1)\n",
        "new_test[['Total Outages']] = y_test_mean\n",
        "\n",
        "# Scaling\n",
        "# Train\n",
        "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_scaled_data = X_scaler.fit_transform(new_train[['Temp', 'Preci', 'Gust', 'Wind', 'Windir', 'LAI',\n",
        "                                      'Total Outages']])\n",
        "Y_scaled_data = Y_scaler.fit_transform(new_train[['Total Outages']])\n",
        "\n",
        "# Validation\n",
        "X_val_scaled_data = X_scaler.transform(new_val[['Temp', 'Preci', 'Gust', 'Wind', 'Windir', 'LAI',\n",
        "                                      'Total Outages']])\n",
        "Y_val_scaled_data = Y_scaler.transform(new_val[['Total Outages']]) \n",
        "\n",
        "# Test\n",
        "X_test_scaled_data = X_scaler.transform(new_test[['Temp', 'Preci', 'Gust', 'Wind', 'Windir', 'LAI',\n",
        "                                      'Total Outages']])\n",
        "Y_test_scaled_data = Y_scaler.transform(new_test[['Total Outages']]) \n",
        "\n",
        "lag = 6 # look back\n",
        "h = 6 # forecast horizon\n",
        "epoch = 100\n",
        "\n",
        "# Call the supervised learning method\n",
        "x_train, y_train = sup_struct_future(X_scaled_data,Y_scaled_data,0,lag,h)\n",
        "x_vali, y_vali = sup_struct_future(X_val_scaled_data,Y_val_scaled_data,0,lag,h)\n",
        "\n",
        "# Reshape arrays for CNN-LSTM\n",
        "x_train_reshaped = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
        "x_vali_reshaped = x_vali.reshape(x_vali.shape[0],x_vali.shape[1],1)\n",
        "y_train_reshaped = y_train.reshape(y_train.shape[0],y_train.shape[1]*1)\n",
        "y_vali_reshaped = y_vali.reshape(y_vali.shape[0],y_vali.shape[1]*1)\n",
        "\n",
        "# Build CNN and LSTM networks\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, Callback, TensorBoard, ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Lambda, Flatten, Activation, Concatenate\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers import Conv1D,CuDNNLSTM\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot\n",
        "from matplotlib import pyplot\n",
        "\n",
        "tf.keras.backend.clear_session() # clears keras session\n",
        "checkpoint_filepath = 'source.h5'\n",
        "inputs1=Input(shape=(x_train_reshaped.shape[-2:]))\n",
        "\n",
        "# CNN Network\n",
        "x = Conv1D(filters=32,kernel_size=2,padding='same',activation=\"relu\")(inputs1)\n",
        "x = Conv1D(filters=32,kernel_size=2,padding='same',activation=\"relu\")(x)\n",
        "x = Conv1D(filters=32,kernel_size=2,padding='same',activation=\"relu\")(x)\n",
        "x = Conv1D(filters=32,kernel_size=2,padding='same',activation=\"relu\")(x)\n",
        "x = MaxPooling1D(pool_size=2,padding='same')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "# LSTM network\n",
        "ls = CuDNNLSTM(50)(inputs1)\n",
        "ls = Activation('relu')(ls)  \n",
        "ls = Dropout(0.1)(ls)\n",
        "\n",
        "# Concatenate outputs from CNN and LSTM networks  \n",
        "merged = Concatenate()([x, ls])   \n",
        "# Pass outputs to dense and lamda layers for optimization\n",
        "merged = Dense(10,activation='relu')(merged) \n",
        "merged = Dense(h,activation='linear')(merged)\n",
        "merged = Lambda(lambda x: x * 10)(merged)  \n",
        "model=Model(inputs1,merged)\n",
        "\n",
        "# Optimizer and loss function \n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-6, momentum = 0.9)\n",
        "model.compile(loss=tf.keras.losses.Huber(),optimizer=optimizer)\n",
        "\n",
        "# Learning rate schedules  \n",
        "es = [EarlyStopping(monitor=\"val_loss\", patience = 100, mode = 'min', verbose =1)]   \n",
        "mc = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min', \n",
        "                      verbose=0, save_best_only=True) \n",
        "reduce_lr_n = ReduceLROnPlateau(monitor='loss', factor=0.8,\n",
        "                            patience=5, min_lr=0.0000001, verbose=0,mode='min')\n",
        "callbacks=[es,mc,reduce_lr_n]\n",
        "# Fit \n",
        "history = model.fit(x_train_reshaped,y_train_reshaped,validation_data=(x_vali_reshaped,\n",
        "                    y_vali_reshaped),epochs=epoch,verbose=1,callbacks=callbacks)\n",
        "# Save model architecture\n",
        "dot_img_file = 'source.png'\n",
        "tf.keras.utils.plot_model(model,to_file=dot_img_file, show_shapes=True)\n",
        "\n",
        "# Visualize learning curves   \n",
        "pyplot.title('Learning Curves')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='val')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "# Save best model\n",
        "model.save(checkpoint_filepath)\n",
        "# load the saved model\n",
        "model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "# Prepare test data\n",
        "predictions = []\n",
        "all_preds = []\n",
        "x_future = X_test_scaled_data[:,:-1]\n",
        "end = None\n",
        "start = 0\n",
        "X = []\n",
        "y = []\n",
        "X_f = []\n",
        "start = start + lag\n",
        "if end is None:\n",
        "  end = len(X_test_scaled_data) - h\n",
        "for i in range(start, end, lag):\n",
        "  indices = range(i-lag, i)\n",
        "  X.append(X_test_scaled_data[indices])  \n",
        "  indicey = range(i+1, i+1+h)\n",
        "  y_future = Y_test_scaled_data[indicey] \n",
        "  y_future = y_future.reshape(y_future.shape[1],y_future.shape[0]) \n",
        "  y.append(y_future)\n",
        "  X_f.append(x_future[indicey])\n",
        "  x_past = np.array(X)\n",
        "  x_future_preds = np.array(X_f)\n",
        " \n",
        " \n",
        "  x_past_reshaped = x_past.reshape(x_past.shape[0],x_past.shape[1]*x_past.shape[2])\n",
        "  x_future_preds_reshaped = x_future_preds.reshape(x_future_preds.shape[0],\n",
        "                                                x_future_preds.shape[1]*x_future_preds.shape[2])\n",
        "  x_covs = np.concatenate([x_past_reshaped,x_future_preds_reshaped],axis=1)\n",
        "  x_covs_reshaped = x_covs.reshape(x_covs.shape[0],x_covs.shape[1],1)\n",
        "  y_pred = model.predict(x_covs_reshaped)\n",
        "  predictions.append(y_pred) \n",
        "  X = []  \n",
        "  X_f = []\n",
        "\n",
        "# Compute test scores\n",
        "y_target = np.array(y)\n",
        "y_p = np.array(predictions)\n",
        "y_last_h_reshaped = y_p.reshape(y_p.shape[0],y_p.shape[1]*y_p.shape[2])\n",
        "y_last_h =y_last_h_reshaped[:,-1]\n",
        "yhat = y_last_h.reshape(-1,1)\n",
        "pred_Inverse = Y_scaler.inverse_transform(yhat)\n",
        "# Inverse power transform of forecast values\n",
        "y_b_inv = [invert_boxcox(x, lmbda) for x in pred_Inverse]\n",
        "y_b_inv = np.array(y_b_inv)\n",
        "# Inverse power transform of actual values\n",
        "y_test_reshaped = y_target.reshape(y_target.shape[0],y_target.shape[1]*y_target.shape[2])\n",
        "y_test_actual = y_test_reshaped[:,-1]\n",
        "y_test_actual = y_test_actual.reshape(-1,1)\n",
        "actual_Inverse = Y_scaler.inverse_transform(y_test_actual) \n",
        "y_actual_inv = [invert_boxcox(x, lmbda) for x in actual_Inverse]\n",
        "y_actual_inv = np.array(y_actual_inv)\n",
        "\n",
        "y_actual = y_actual_inv\n",
        "y_train_actual = raw_train['Total Outages']\n",
        "y_train_actual = y_train_actual.values\n",
        "\n",
        "pred = y_b_inv\n",
        "actual = y_actual[-len(pred):]\n",
        "\n",
        "mse = mean_squared_error(actual, pred)\n",
        "# calculate rmse\n",
        "rmse = sqrt(mse)  \n",
        "# print rmse\n",
        "print('Average Test RMSE: %.3f' % (rmse))\n",
        "from sklearn import metrics\n",
        "print(f'R2 is : {metrics.r2_score(actual, pred)}',end='\\n\\n')\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "fig.tight_layout(pad=6.0)\n",
        "axes[0].set_title(\"Actual & forecasted test samples\")\n",
        "axes[0].set_xlabel(\"Time Step\")\n",
        "axes[0].set_ylabel(\"Customer Outages\")\n",
        "real = DataFrame(actual)\n",
        "real = real.values\n",
        "\n",
        "x_axis = np.arange(y_train_actual.shape[0] + pred.shape[0])\n",
        "axes[0].plot(x_axis[:y_train_actual.shape[0]], y_train_actual, alpha=0.75,label='Train Data')\n",
        "axes[0].legend(loc=\"upper right\")\n",
        "axes[0].scatter(x_axis[y_train_actual.shape[0]:], pred, alpha=0.4,label='Forecast')\n",
        "axes[0].legend(loc=\"upper right\")\n",
        "axes[0].scatter(x_axis[y_train_actual.shape[0]:], y_actual[-len(pred):], alpha=0.4, marker='x',label='Test Data')\n",
        "axes[0].legend(loc=\"upper right\")\n",
        "\n",
        "new_x_axis = np.arange(300)\n",
        "axes[1].plot(new_x_axis[:real.shape[0]], real[:300], alpha=0.75,label='Actual')\n",
        "axes[1].legend(loc=\"upper right\")\n",
        "axes[1].plot(new_x_axis[:real.shape[0]],pred[:300],alpha=0.4,label='Predicted',color='r')\n",
        "axes[1].legend(loc=\"upper right\")\n",
        "axes[1].set_title(\"Actual & forecasted test samples:First 100 time steps in expanded view\")\n",
        "axes[1].set_xlabel(\"Time Step\")\n",
        "axes[1].set_ylabel(\"Customer Outages\")\n",
        "plt.show()"
      ]
    }
  ]
}